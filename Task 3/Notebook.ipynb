{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gnU1zZl0myXM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Tables preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i8zMF7dFm2gU"
   },
   "outputs": [],
   "source": [
    "# Function to read csv files.\n",
    "def read_file_csv(file_name):\n",
    "    return pd.read_csv(file_name, encoding=\"ISO-8859-1\")\n",
    "\n",
    "\n",
    "# Our data.\n",
    "books_df = read_file_csv('./data/books.csv')\n",
    "books_tags_df = read_file_csv('./data/books_tags.csv')\n",
    "\n",
    "users_df = read_file_csv('./data/users.csv')\n",
    "ratings_df = read_file_csv('./data/ratings.csv')\n",
    "tags_df = read_file_csv('./data/tags.csv')\n",
    "\n",
    "test_df = read_file_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "95CjEqkJnBcQ",
    "outputId": "382484ca-989b-4e49-8c56-385971669aba"
   },
   "outputs": [],
   "source": [
    "# Book with ratings and users dataframe.\n",
    "global b_r_u_df\n",
    "b_r_u_df = pd.merge(books_df[['book_id', 'title']], ratings_df, on='book_id', how='inner')\n",
    "b_r_u_df = pd.merge(b_r_u_df, users_df, on='user_id', how='inner')\n",
    "\n",
    "global items_df\n",
    "items_df = b_r_u_df[['book_id', 'title']].drop_duplicates(subset='book_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Personalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for each book. Explained in the report.\n",
    "def weighted_average_rating(counts_df, m, rating_mean_df, rating_all_mean):\n",
    "    return (counts_df / (counts_df + m)) * rating_mean_df + (m / (counts_df + m)) * rating_all_mean\n",
    "\n",
    "\n",
    "# To get the top k recommendations. The k books with the best score.\n",
    "def get_top_k_recommendations(df, counts_df, k, columns, m):\n",
    "    return df[counts_df >= m].nlargest(k, columns)\n",
    "\n",
    "# We'll need this function for the recommendations according to the age.\n",
    "# Gets a number and creates the range of ages as required in the task.\n",
    "def get_age_range(age):\n",
    "    if age % 10 == 0:\n",
    "        age -= 1\n",
    "\n",
    "    lower_bound = age - ((age % 10) - 1)\n",
    "    upper_bound = age + (10 - (age % 10))\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "# Creates distribution of votes and ratings by book id and returns them.\n",
    "def merge_tables(df):\n",
    "    # Dataframe that contains distribution of votes by book ID.\n",
    "    nb_voters_data = df['book_id'].value_counts()\n",
    "    nb_voters_df = pd.DataFrame(data={'book_id': nb_voters_data.index.tolist(), 'counts': nb_voters_data.values.tolist()})\n",
    "\n",
    "    # Dataframe that contains distribution of rate averages by book ID.\n",
    "    rating_mean_data = df.groupby(['book_id'])['rating'].mean()\n",
    "    rating_mean_df = pd.DataFrame(data={'book_id': rating_mean_data.index.tolist(), 'rating_mean': rating_mean_data.values.tolist()})\n",
    "    \n",
    "    return nb_voters_df, nb_voters_data, rating_mean_df, rating_mean_data\n",
    "\n",
    "\n",
    "# m represents the minimum voters we need to count the rating and the score. \n",
    "# We'll also need the total mean to caluculate the score (WR).\n",
    "def get_voters_and_means(nb_voters_data, rating_mean_df):\n",
    "    m = nb_voters_data.quantile(0.90)\n",
    "    rating_all_mean = rating_mean_df['rating_mean'].mean()\n",
    "    return m, rating_all_mean\n",
    "\n",
    "\n",
    "# Get the top k recommendations: the k books with the best weighted average rating.\n",
    "def calculate_WAR_and_top_k(df, m, rating_all_mean, k, columns):\n",
    "    df['weighted_average_rating'] = weighted_average_rating(df['counts'], m, df['rating_mean'], rating_all_mean)\n",
    "    return get_top_k_recommendations(df, df['counts'], k, columns, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_simply_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     book_id                                              title  \\\n",
      "504       25  Harry Potter and the Deathly Hallows (Harry Po...   \n",
      "1          4                              To Kill a Mockingbird   \n",
      "506      102                          Where the Wild Things Are   \n",
      "370       85                                    The Giving Tree   \n",
      "364       50                            Where the Sidewalk Ends   \n",
      "8         31                                           The Help   \n",
      "425      144  Unbroken: A World War II Story of Survival, Re...   \n",
      "212       27  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "0          1            The Hunger Games (The Hunger Games, #1)   \n",
      "663      133    Anne of Green Gables (Anne of Green Gables, #1)   \n",
      "\n",
      "     weighted_average_rating  \n",
      "504                 4.338028  \n",
      "1                   4.299843  \n",
      "506                 4.273212  \n",
      "370                 4.240309  \n",
      "364                 4.239724  \n",
      "8                   4.238851  \n",
      "425                 4.221864  \n",
      "212                 4.213906  \n",
      "0                   4.187383  \n",
      "663                 4.181489  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the WAR and get the k books with the best results.\n",
    "def get_simply_recommendation(k):\n",
    "    global b_r_u_df\n",
    "    \n",
    "    nb_voters_df, nb_voters_data, rating_mean_df, rating_mean_data = merge_tables(b_r_u_df)\n",
    "    m, rating_all_mean = get_voters_and_means(nb_voters_data, rating_mean_df)\n",
    "    \n",
    "    df = pd.merge(items_df, nb_voters_df, on='book_id', how='inner')\n",
    "    df = pd.merge(df, rating_mean_df, on='book_id', how='inner')\n",
    "\n",
    "    return calculate_WAR_and_top_k(df, m, rating_all_mean, k, ['weighted_average_rating'])\n",
    "\n",
    "\n",
    "recommendation_df = get_simply_recommendation(10)\n",
    "print(recommendation_df[['book_id','title','weighted_average_rating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_simply_place_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      book_id                                              title  \\\n",
      "310       126                          Dune (Dune Chronicles #1)   \n",
      "602       143                        All the Light We Cannot See   \n",
      "356       144  Unbroken: A World War II Story of Survival, Re...   \n",
      "190        24  Harry Potter and the Goblet of Fire (Harry Pot...   \n",
      "424       102                          Where the Wild Things Are   \n",
      "889       490  Maus I: A Survivor's Tale: My Father Bleeds Hi...   \n",
      "1191     1462                            The Orphan Master's Son   \n",
      "1554      983                           Between the World and Me   \n",
      "884       119                                The Handmaid's Tale   \n",
      "800        89                                The Princess Bride    \n",
      "\n",
      "      weighted_average_rating  \n",
      "310                  4.367963  \n",
      "602                  4.317786  \n",
      "356                  4.266087  \n",
      "190                  4.249728  \n",
      "424                  4.226877  \n",
      "889                  4.213664  \n",
      "1191                 4.213664  \n",
      "1554                 4.213664  \n",
      "884                  4.199565  \n",
      "800                  4.190062  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the WAR and get the k best books by a place.\n",
    "def get_simply_place_recommendation(place, k):\n",
    "    global b_r_u_df\n",
    "    \n",
    "    b_r_u_place_df = b_r_u_df[b_r_u_df['location'] == place]\n",
    "    \n",
    "    nb_voters_df, nb_voters_data, rating_mean_df, rating_mean_data = merge_tables(b_r_u_place_df)\n",
    "    m, rating_all_mean = get_voters_and_means(nb_voters_data, rating_mean_df)\n",
    "    \n",
    "    df = pd.merge(items_df, nb_voters_df, on='book_id', how='inner')\n",
    "    df = pd.merge(df, rating_mean_df, on='book_id', how='inner')\n",
    "\n",
    "    return calculate_WAR_and_top_k(df, m, rating_all_mean, k, ['weighted_average_rating'])\n",
    "\n",
    "\n",
    "place_recommendation_df = get_simply_place_recommendation('Ohio', 10)\n",
    "print(place_recommendation_df[['book_id','title','weighted_average_rating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_simply_age_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     book_id                                              title  \\\n",
      "502       25  Harry Potter and the Deathly Hallows (Harry Po...   \n",
      "1          4                              To Kill a Mockingbird   \n",
      "368       85                                    The Giving Tree   \n",
      "948       89                                The Princess Bride    \n",
      "659      133    Anne of Green Gables (Anne of Green Gables, #1)   \n",
      "362       50                            Where the Sidewalk Ends   \n",
      "504      102                          Where the Wild Things Are   \n",
      "418       70                    Ender's Game (Ender's Saga, #1)   \n",
      "8         31                                           The Help   \n",
      "209       21  Harry Potter and the Order of the Phoenix (Har...   \n",
      "\n",
      "     weighted_average_rating  \n",
      "502                 4.326251  \n",
      "1                   4.294203  \n",
      "368                 4.289614  \n",
      "948                 4.244702  \n",
      "659                 4.224914  \n",
      "362                 4.216411  \n",
      "504                 4.204680  \n",
      "418                 4.204095  \n",
      "8                   4.202891  \n",
      "209                 4.196385  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the WAR and get the k best books by a range of ages.\n",
    "def get_simply_age_recommendation(age, k):\n",
    "    global b_r_u_df\n",
    "    \n",
    "    lower_bound, upper_bound = get_age_range(age)\n",
    "    b_r_u_age_df = b_r_u_df[(b_r_u_df['age'] >= lower_bound) & (b_r_u_df['age'] <= upper_bound)]\n",
    "    \n",
    "    nb_voters_df, nb_voters_data, rating_mean_df, rating_mean_data = merge_tables(b_r_u_age_df)\n",
    "    m, rating_all_mean = get_voters_and_means(nb_voters_data, rating_mean_df)\n",
    "    \n",
    "    df = pd.merge(items_df, nb_voters_df, on='book_id', how='inner')\n",
    "    df = pd.merge(df, rating_mean_df, on='book_id', how='inner')\n",
    "\n",
    "    return calculate_WAR_and_top_k(df, m, rating_all_mean, k, ['weighted_average_rating'])\n",
    "\n",
    "\n",
    "age_recommendation_df = get_simply_age_recommendation(28, 10)\n",
    "print(age_recommendation_df[['book_id','title','weighted_average_rating']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top K values from given array.\n",
    "def keep_top_k(array, k):\n",
    "    smallest = heapq.nlargest(k, array)[-1]\n",
    "    array[array < smallest] = 0\n",
    "    return array\n",
    "\n",
    "\n",
    "# Similirity matrix according to the chosen similarity.\n",
    "def build_CF_prediction_matrix(sim):\n",
    "    global ratings_diff\n",
    "    return 1-pairwise_distances(ratings_diff, metric=sim)\n",
    "\n",
    "\n",
    "# Function that extracts user recommendations.\n",
    "# Gets the highest rates indexes and returns book id and the title for that user.\n",
    "def get_CF_final_output(pred, data_matrix, user_id, items_new_to_original, items, k):\n",
    "    user_id = user_id - 1\n",
    "    predicted_ratings_row = pred[user_id]\n",
    "    data_matrix_row = data_matrix[user_id]\n",
    "    \n",
    "    predicted_ratings_unrated = predicted_ratings_row.copy()\n",
    "    predicted_ratings_unrated[~np.isnan(data_matrix_row)] = 0\n",
    "\n",
    "    idx = np.argsort(-predicted_ratings_unrated)\n",
    "    sim_scores = idx[0:k]\n",
    "    \n",
    "    # When getting the results, we map them back to original book ids.\n",
    "    books_original_indexes_df = pd.DataFrame(data={'book_id': [items_new_to_original[index] for index in sim_scores]})\n",
    "    return pd.merge(books_original_indexes_df, items, on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the collaborative filtering recommendation according to the user.\n",
    "def get_CF_recommendation(user_id, k):\n",
    "    # Import global variables.\n",
    "    global ratings_df\n",
    "    global items_df\n",
    "\n",
    "    # Declare of global variables.\n",
    "    global users_original_to_new\n",
    "    global users_new_to_original\n",
    "    global items_original_to_new\n",
    "    global items_new_to_original\n",
    "    global data_matrix\n",
    "    global mean_user_rating\n",
    "\n",
    "    # Part 1.\n",
    "    unique_users = ratings_df['user_id'].unique()\n",
    "    unique_items = ratings_df['book_id'].unique()\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_items = unique_items.shape[0]\n",
    "\n",
    "    # Working on user data.\n",
    "    unique_users.sort()\n",
    "    # Creating a dictionary that contains a mapping from original user id to a new user id.\n",
    "    users_original_to_new = {original_index: new_index for original_index, new_index in zip(unique_users, range(n_users))}\n",
    "    # Creating a dictionary that contains a mapping from new user id to a original user id.\n",
    "    users_new_to_original = {value: key for key, value in users_original_to_new.items()}\n",
    "\n",
    "    # Working on items data.\n",
    "    unique_items.sort()\n",
    "    # Creating a dictionary that contains a mapping from original book id to a new book id.\n",
    "    items_original_to_new = {original_index: new_index for original_index, new_index in zip(unique_items, range(n_items))}\n",
    "    # Creating a dictionary that contains a mapping from new book id to a original book id.\n",
    "    items_new_to_original = {value: key for key, value in items_original_to_new.items()}\n",
    "\n",
    "    # Part 2.\n",
    "    data_matrix = np.empty((n_users, n_items))\n",
    "    data_matrix[:] = np.nan\n",
    "    for line in ratings_df.itertuples():\n",
    "        user = users_original_to_new[line[1]]\n",
    "        book = items_original_to_new[line[2]]\n",
    "        rating = line[3]\n",
    "        data_matrix[user, book] = rating\n",
    "\n",
    "    mean_user_rating = np.nanmean(data_matrix, axis=1).reshape(-1, 1)\n",
    "\n",
    "    global ratings_diff\n",
    "    ratings_diff = (data_matrix - mean_user_rating)\n",
    "    ratings_diff[np.isnan(ratings_diff)] = 0\n",
    "\n",
    "    user_similarity = build_CF_prediction_matrix('cosine')\n",
    "    user_similarity = np.array([keep_top_k(np.array(arr), k) for arr in user_similarity])\n",
    "    pred = mean_user_rating + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "\n",
    "    # Part 3.\n",
    "    return get_CF_final_output(pred, data_matrix, user_id, items_new_to_original, items_df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book_id                                title\n",
      "0      101               Me Talk Pretty One Day\n",
      "1      775                            Just Kids\n",
      "2      264                   The Sun Also Rises\n",
      "3      289  Watership Down (Watership Down, #1)\n",
      "4      335            James and the Giant Peach\n",
      "5     1084                    To the Lighthouse\n",
      "6      468         Their Eyes Were Watching God\n",
      "7      184                              Matilda\n",
      "8       83                 A Tale of Two Cities\n",
      "9      344                                Naked\n"
     ]
    }
   ],
   "source": [
    "recommendations_by_user = get_CF_recommendation(user_id=1, k=10)\n",
    "print(recommendations_by_user.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataframe for the algorithm.\n",
    "# Reading the tags and the relevant features and preparing the features for the algorithm.\n",
    "bookreads_tags_df = pd.merge(books_tags_df, tags_df, on='tag_id', how='inner')\n",
    "\n",
    "groupped_data = bookreads_tags_df.groupby('goodreads_book_id', as_index=False)['tag_name'].transform(lambda x: ' '.join(x))\n",
    "books_tags_row_df = pd.DataFrame(data={'goodreads_book_id': groupped_data.index.tolist(), 'tag_name': groupped_data['tag_name'].values.tolist()})\n",
    "\n",
    "global contact_based_filtering_df\n",
    "contact_based_filtering_df = pd.merge(books_df[['book_id', 'title', 'authors', 'goodreads_book_id',  'language_code', 'original_title']], books_tags_row_df, on='goodreads_book_id', how='outer')\n",
    "contact_based_filtering_df['tag_name'] = contact_based_filtering_df['tag_name'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data to get lower case letter and get rid of '-'.\n",
    "def clean_data(x):\n",
    "    x = str.lower(str(x))\n",
    "    return x.replace('-', '')\n",
    "\n",
    "\n",
    "# Get all of our features together with a space in between. The choice of the features is explained in the report.\n",
    "def create_soup(x):\n",
    "    return x['original_title'] + ' ' + x['language_code'] + ' ' + x['authors'] + ' ' + x['tag_name']\n",
    "\n",
    "\n",
    "# Similarity matrix. We use cosine similarity\n",
    "def build_contact_sim_metrix():\n",
    "    global count_matrix\n",
    "    return cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We return the top k recommendation according to the content (The features of each book).\n",
    "def get_contact_recommendation(book_name, k):\n",
    "    global contact_based_filtering_df\n",
    "\n",
    "    features = ['original_title', 'language_code', 'authors', 'tag_name']\n",
    "    for feature in features:\n",
    "        contact_based_filtering_df[feature] = contact_based_filtering_df[feature].apply(clean_data)\n",
    "\n",
    "    contact_based_filtering_df['soup'] = contact_based_filtering_df.apply(create_soup, axis=1)\n",
    "\n",
    "    global count_matrix\n",
    "    count_matrix = CountVectorizer(stop_words='english').fit_transform(contact_based_filtering_df['soup'])\n",
    "\n",
    "    cosine_sim = build_contact_sim_metrix()\n",
    "\n",
    "    contact_based_filtering_df = contact_based_filtering_df.reset_index()\n",
    "    indices = pd.Series(contact_based_filtering_df.index, index=contact_based_filtering_df['title'])\n",
    "\n",
    "    idx = indices[book_name]\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:k+1]\n",
    "\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    return contact_based_filtering_df['title'].iloc[book_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4052    The Twilight Saga: The Official Illustrated Gu...\n",
      "51                                 Eclipse (Twilight, #3)\n",
      "72                                The Host (The Host, #1)\n",
      "3059    Twilight: The Graphic Novel, Vol. 1 (Twilight:...\n",
      "55                           Breaking Dawn (Twilight, #4)\n",
      "2015             The Twilight Collection (Twilight, #1-3)\n",
      "48                                New Moon (Twilight, #2)\n",
      "990                    The Twilight Saga (Twilight, #1-4)\n",
      "1613    The Twilight Saga Complete Collection  (Twilig...\n",
      "1519          Crossroads of Twilight (Wheel of Time, #10)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "contact_based_filtering_result = get_contact_recommendation('Twilight (Twilight, #1)', k=10)\n",
    "print(contact_based_filtering_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 - Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data. We want only the books with ratings >= 4 and the users that that ranked books at least 10 times.\n",
    "high_rate_test_df = test_df[test_df['rating'] >= 4]\n",
    "user_value_counts = high_rate_test_df['user_id'].value_counts()\n",
    "user_value_counts_df = pd.DataFrame(data={'user_id': user_value_counts.index.tolist(), 'appearances': user_value_counts.values.tolist()})\n",
    "user_value_counts_df = user_value_counts_df[user_value_counts_df['appearances'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-mobini\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08, 0.008, 0.08]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation P@k for each similarity.\n",
    "def precision_k(k):\n",
    "    # Importing global variables that has been assigned before to use them here.\n",
    "    global items_df\n",
    "    global data_matrix\n",
    "    global ratings_diff\n",
    "    global mean_user_rating\n",
    "    global items_new_to_original\n",
    "    \n",
    "    pk_list = []\n",
    "    for sim in ['cosine', 'euclidean', 'jaccard']:\n",
    "        calculations_list = []\n",
    "\n",
    "        user_similarity = 1-pairwise_distances(ratings_diff, metric=sim)\n",
    "        user_similarity = np.array([keep_top_k(np.array(arr), k) for arr in user_similarity])\n",
    "        pred = mean_user_rating + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "\n",
    "        for user_id in user_value_counts_df['user_id'].values:\n",
    "            user_recommendations = get_CF_final_output(pred, data_matrix, user_id, items_new_to_original, items_df, k)\n",
    "\n",
    "            counter = 0\n",
    "            for book_idx in user_recommendations['book_id'].values:\n",
    "                chosen_book = high_rate_test_df[(high_rate_test_df['user_id'] == user_id) & (high_rate_test_df['book_id'] == book_idx)]\n",
    "                if chosen_book.shape[0] == 1:\n",
    "                    counter += 1\n",
    "\n",
    "            calculations_list.append(counter / k)\n",
    "\n",
    "        pk_list.append(sum(calculations_list) / user_value_counts_df.shape[0])\n",
    "\n",
    "    return pk_list\n",
    "\n",
    "\n",
    "print(precision_k(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-mobini\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6466666666666666, 0.08, 0.6266666666666666]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation ARHR.\n",
    "def ARHR(k):\n",
    "    # Importing global variables that has been assigned before to use them here.\n",
    "    global items_df\n",
    "    global data_matrix\n",
    "    global ratings_diff\n",
    "    global mean_user_rating\n",
    "    global items_new_to_original\n",
    "    \n",
    "    arhr_list = []\n",
    "    for sim in ['cosine', 'euclidean', 'jaccard']:\n",
    "        calculations_list = []\n",
    "\n",
    "        user_similarity = 1-pairwise_distances(ratings_diff, metric=sim)\n",
    "        user_similarity = np.array([keep_top_k(np.array(arr), k) for arr in user_similarity])\n",
    "        pred = mean_user_rating + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "\n",
    "        for user_id in user_value_counts_df['user_id'].values:\n",
    "            user_recommendations = get_CF_final_output(pred, data_matrix, user_id, items_new_to_original, items_df, k)\n",
    "\n",
    "            user_high_rate_df = high_rate_test_df[high_rate_test_df['user_id'] == user_id]\n",
    "            user_rec_merged_df = pd.merge(user_recommendations, user_high_rate_df, on='book_id', how='inner')\n",
    "\n",
    "            for position in user_rec_merged_df.index + 1:\n",
    "                calculations_list.append(1 / position)\n",
    "\n",
    "        arhr_list.append(sum(calculations_list) / user_value_counts_df.shape[0])\n",
    "\n",
    "    return arhr_list\n",
    "\n",
    "\n",
    "print(ARHR(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t-mobini\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9176794695882072, 0.9168899103744533, 0.9187216947055077]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to build the function RMSE. This time we wont filter the data.\n",
    "def get_recommendations_RMSE(pred, data_matrix, user_id):\n",
    "    user_id = user_id - 1\n",
    "    predicted_ratings_row = pred[user_id]\n",
    "    data_matrix_row = data_matrix[user_id]\n",
    "\n",
    "    predicted_ratings_unrated = predicted_ratings_row.copy()\n",
    "    predicted_ratings_unrated[~np.isnan(data_matrix_row)] = 0\n",
    "\n",
    "    book_ids = np.argsort(-predicted_ratings_unrated)\n",
    "    books_rating = np.sort(predicted_ratings_unrated)[::-1]\n",
    "\n",
    "    return {idx: rating for idx, rating in zip(book_ids, books_rating)}\n",
    "\n",
    "\n",
    "# Evaluation RMSE.\n",
    "def RMSE():\n",
    "    # Importing global variables that has been assigned before to use them here.\n",
    "    global ratings_diff\n",
    "    global mean_user_rating\n",
    "    \n",
    "    rmse_list = []\n",
    "    for sim in ['cosine', 'euclidean', 'jaccard']:\n",
    "        sum_error = 0\n",
    "        count_lines = 0\n",
    "\n",
    "        user_similarity = 1-pairwise_distances(ratings_diff, metric=sim)\n",
    "        pred = mean_user_rating + user_similarity.dot(ratings_diff) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "\n",
    "        for user_id, test_user_data in test_df.groupby('user_id'):\n",
    "            user_recommendations = get_recommendations_RMSE(pred, data_matrix, user_id)\n",
    "\n",
    "            for row in test_user_data.itertuples(index=False):\n",
    "                _, test_book_id, rating = tuple(row)\n",
    "                prediction = user_recommendations[test_book_id] if test_book_id in user_recommendations else 0\n",
    "\n",
    "                if prediction == 0:\n",
    "                    continue\n",
    "\n",
    "                sum_error += (prediction - rating)**2\n",
    "                count_lines += 1\n",
    "\n",
    "        rmse_list.append(math.sqrt(sum_error/count_lines))\n",
    "\n",
    "    return rmse_list\n",
    "    \n",
    "    \n",
    "print(RMSE())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
